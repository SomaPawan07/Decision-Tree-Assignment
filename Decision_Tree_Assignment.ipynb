{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is a Decision Tree, and how does it work in the context of classification ?**\n",
        "- A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. In the context of classification, it's a flowchart-like structure that helps make decisions based on input features to predict a class label.\n",
        "The algorithm follows a divide-and-conquer approach:\n",
        "1. Feature Selection: The algorithm selects the best feature to split the dataset based on a criterion (like Gini Impurity, Entropy/Information Gain, or Chi-square).\n",
        "\n",
        "2. Splitting: The data is split into subsets based on the selected feature's values.\n",
        "\n",
        "3. Recursion: The process is repeated recursively for each subset until: All data in a node belongs to the same class, or A stopping criterion is met (e.g., max depth, minimum samples, etc.)\n",
        "\n",
        "4. Prediction: To classify a new instance, you start at the root and follow the branches according to the feature values of the instance until you reach a leaf node with the predicted class."
      ],
      "metadata": {
        "id": "N6yTrV0SvHTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree ?**\n",
        "- An impurity measure quantifies how mixed the class labels are at a particular node.\n",
        "1. Gini Impurity :- Gini Impurity is used in the CART algorithm.\n",
        "  Where:\n",
        "pi= proportion of instances belonging to class\n",
        "C = number of classes\n",
        "Example: Suppose a node contains 3 samples:\n",
        "2 of class A\n",
        "1 of class B\n",
        "\n",
        "Then:\n",
        "                      Gini=1−(2/3)2−(1/3)2=1−4/9−1/9=4/9≈0.444\n",
        "If all samples were of class:\n",
        "                      Gini=1−(1)2=0(pure)\n",
        "\n",
        "2. Entropy (Information Gain) : Used in the ID3 and C4.5 algorithms.                     \n",
        "Formula:\n",
        "                       Entropy=−i=1∑C​pi​log2​(pi​)\n",
        "\n",
        "They impact At each node:\n",
        "- The algorithm evaluates all possible splits for each feature.\n",
        "- For each split, it calculates the weighted average impurity (Gini or Entropy) of the child nodes.\n",
        "- It selects the split that minimizes the weighted impurity — i.e., the split that makes the data more pure.\n",
        "\n",
        "This is called:\n",
        "Gini Gain for Gini Impurity\n",
        "Information Gain for Entropy    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-FONkw-JwCNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "- The difference between Pre-Pruning and Post-Pruning in Decision Trees are as follows:\n",
        "\n",
        "| Feature           | Pre-Pruning                   | Post-Pruning                          |\n",
        "| ----------------- | ----------------------------- | ------------------------------------- |\n",
        "| When Applied      | During tree building          | After full tree is built              |\n",
        "| Control           | Stops growth early            | Trims unnecessary branches            |\n",
        "| Complexity        | Simpler implementation        | Can be more computationally expensive |\n",
        "| Risk              | May stop too early (underfit) | May overfit before pruning            |\n",
        "| Practical Benefit | Faster training               | Better generalization accuracy        |\n",
        "\n",
        "1. Pre-Pruning (a.k.a Early Stopping):- Pre-pruning stops the tree from growing once a condition is met during tree construction.\n",
        "Practical Advantage: Faster training time — because it avoids growing unnecessary parts of the tree.\n",
        "Example: If a split leads to very little information gain, you stop splitting further — even if you could.\n",
        "\n",
        "2. Post-Pruning (a.k.a Pruning after Training):- Post-pruning allows the tree to fully grow, then removes branches that do not contribute much to accuracy.\n",
        "Practical Advantage: Improves accuracy and generalization — by removing overfitting parts after evaluating the whole tree structure.\n",
        "Example:After building the full tree, you test subtrees on a validation set. If pruning them doesn’t reduce accuracy, they are removed."
      ],
      "metadata": {
        "id": "t6eoHZbRylO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "- Information Gain (IG) is a measure used to determine how well a feature splits the data into distinct classes in a decision tree. It quantifies the reduction in uncertainty (entropy) after splitting a dataset based on a particular feature.\n",
        "It is important for choosing the best split because:\n",
        "In decision tree algorithms (like ID3 and C4.5), choosing the best split at each node is critical.\n",
        "\n",
        "Information Gain tells us: \"How much better are we at classifying the data if we split on this feature?\"\n",
        "\n",
        "The feature that maximizes information gain is selected for the split — because it best reduces disorder in the data."
      ],
      "metadata": {
        "id": "umCLm4rxz_so"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?**\n",
        "- Real-World Applications of Decision Trees\n",
        "1. Medical Diagnosis\n",
        "Use case: Diagnosing diseases based on symptoms and patient history.\n",
        "Example: Classifying whether a tumor is benign or malignant.\n",
        "2. Credit Scoring and Loan Approval\n",
        "Use case: Evaluating whether a customer is eligible for a loan.\n",
        "Example: Splitting on income, employment status, credit history.\n",
        "3. Fraud Detection\n",
        "Use case: Identifying unusual transactions or behavior.\n",
        "Example: Splitting on transaction amount, location, frequency.\n",
        "4. Customer Churn Prediction\n",
        "Use case: Predicting if a customer is likely to stop using a service.\n",
        "Example: Splitting on service usage, complaints, tenure.\n",
        "5. Marketing and Recommendation Systems\n",
        "Use case: Segmenting customers and predicting product preferences.\n",
        "Example: Predicting if a user will click on an ad based on behavior.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "| Advantage                              | Description                                                      |\n",
        "| -------------------------------------- | ---------------------------------------------------------------- |\n",
        "| **Easy to Understand**                 | Intuitive, similar to decision-making processes humans use.      |\n",
        "| **No Data Scaling Required**           | Works without normalization or standardization.                  |\n",
        "| **Handles Both Data Types**            | Works with both numerical and categorical variables.             |\n",
        "| **Feature Selection Built-in**         | Automatically selects the most important features during splits. |\n",
        "| **Can Model Non-linear Relationships** | Captures complex decision boundaries.                            |\n",
        "\n",
        "Limitations:\n",
        "\n",
        "| Limitation                           | Description                                                              |\n",
        "| ------------------------------------ | ------------------------------------------------------------------------ |\n",
        "| **Overfitting**                      | Trees can grow very deep and memorize training data.                     |\n",
        "| **Unstable**                         | Small changes in data can lead to a completely different tree structure. |\n",
        "| **Biased to Dominant Features**      | Features with more levels may dominate the splits.                       |\n",
        "| **Greedy Algorithm**                 | Makes locally optimal choices that may not lead to a globally best tree. |\n",
        "| **Poor Performance with Noisy Data** | Sensitive to outliers and irrelevant features.                           |\n",
        "\n"
      ],
      "metadata": {
        "id": "HrbTUijU03yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Dataset Info:\n",
        "\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "**Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances**"
      ],
      "metadata": {
        "id": "yS7SSSeI1onI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypnGkvYs2XjW",
        "outputId": "12e610bf-e350-4b6b-f703-2dcac42f97d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.**"
      ],
      "metadata": {
        "id": "6KLDEWF-2pzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "tree_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = tree_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_depth3:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqn62v5128qU",
        "outputId": "beef7ca9-e520-4b48-9784-bc8402395973"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy with fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances.**"
      ],
      "metadata": {
        "id": "Zi7oIEGe3JFy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jwlu5vyvGEW",
        "outputId": "32ec741c-ba29-4183-ffea-e7bdbfa23c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4952\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "california = fetch_california_housing()\n",
        "X = california.data\n",
        "y = california.target\n",
        "feature_names = california.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. Write a Python program to:**\n",
        "\n",
        "● **Load the Iris Dataset**\n",
        "\n",
        "● **Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV**\n",
        "\n",
        "● **Print the best parameters and the resulting model accuracy**"
      ],
      "metadata": {
        "id": "Yv9WAfqE3ggs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Test Set Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyRvwxbm3li5",
        "outputId": "bbab176b-f7f7-4363-91c8-95118de943ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Test Set Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.**\n",
        "\n",
        "**Explain the step-by-step process you would follow to:**\n",
        "\n",
        "● **Handle the missing values**\n",
        "\n",
        "● **Encode the categorical features**\n",
        "\n",
        "● **Train a Decision Tree model**\n",
        "\n",
        "● **Tune its hyperparameters**\n",
        "\n",
        "● **Evaluate its performance**\n",
        "\n",
        "**And describe what business value this model could provide in the real-worldsetting.**\n",
        "\n",
        "Answer 1. Handle Missing Values\n",
        "\n",
        "Why: Missing data can reduce model performance or cause errors during training.\n",
        "Steps:\n",
        "\n",
        "- Numerical features: Use mean, median, or model-based imputation"
      ],
      "metadata": {
        "id": "y57sgBF4321v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X_num = num_imputer.fit_transform(X_num)\n"
      ],
      "metadata": {
        "id": "LwiDdbsU623-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Categorical features: Use mode or a constant value like \"Missing\""
      ],
      "metadata": {
        "id": "0Lxw_fuL65c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X_cat = cat_imputer.fit_transform(X_cat)\n"
      ],
      "metadata": {
        "id": "dV21E2Bl7DTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Encode Categorical Features\n",
        "Why: Decision Trees in scikit-learn require numerical inputs.\n",
        "Steps:\n",
        "- Use One-Hot Encoding for nominal categories"
      ],
      "metadata": {
        "id": "wnlibpRe7Jb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "X_cat_encoded = encoder.fit_transform(X_cat)\n"
      ],
      "metadata": {
        "id": "6OMEmLeK7PMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Combine with numerical data"
      ],
      "metadata": {
        "id": "Jx1jOKlj7QUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_final = np.hstack((X_num, X_cat_encoded))\n"
      ],
      "metadata": {
        "id": "8i-79egR7Ud4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Train a Decision Tree Model"
      ],
      "metadata": {
        "id": "CvQxv_RG7b36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "SCGreOdP7g3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Tune Hyperparameters\n",
        "Use GridSearchCV to tune key parameters:\n",
        "Common Hyperparameters:\n",
        "- max_depth\n",
        "- min_samples_split\n",
        "- min_samples_leaf\n",
        "- criterion (gini or entropy)\n",
        "\n"
      ],
      "metadata": {
        "id": "5XGvDLYP7txD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n"
      ],
      "metadata": {
        "id": "JDWQh5nQ74gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Evaluate Model Performance\n",
        "Use multiple metrics:"
      ],
      "metadata": {
        "id": "J19axCGb751g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "zGxvzxVa79ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Value:\n",
        "\n",
        "| Value Area                 | Description                                                          |\n",
        "| -------------------------- | -------------------------------------------------------------------- |\n",
        "| **Early Detection**        | Helps detect diseases at early stages, improving treatment outcomes. |\n",
        "| **Decision Support**       | Assists doctors with evidence-based risk assessment.                 |\n",
        "| **Resource Optimization**  | Prioritizes testing or referrals for high-risk patients.             |\n",
        "| **Cost Reduction**         | Avoids unnecessary procedures for low-risk cases.                    |\n",
        "| **Compliance & Reporting** | Provides auditable decision logic for regulatory compliance.         |\n",
        "| **Scalability**            | Automates screening in large populations with minimal manual effort. |\n"
      ],
      "metadata": {
        "id": "2sybEjhQ8D7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xw6_bUzI7tIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0_Lkbvij6pTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ip9JKZn330qJ"
      }
    }
  ]
}